{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание архитектуры нейронной сети посредством модуля torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети состоят из слоев, которые производят преобразования над данными. В PyTorch принято называть слои ***модулями*** (modules), и далее мы тоже будем использовать это название.\n",
    "\n",
    "Для большей чёткости в структуре нейронной сети используются классы. В PyTorch есть отдельный класс [torch.nn](https://pytorch.org/docs/stable/nn.html), специально созданный для работы с нейронными сетями. В нём уже реализованы все типы слоёв и функций активации. Это позволяет существенно сократить код и упростить работу с нейронными сетями.\n",
    "\n",
    "Пространство имен [`torch.nn`](https://pytorch.org/docs/stable/nn.html) предоставляет \"строительные блоки\", которые нужны для создания своей собственной нейронной сети. Каждый *модуль* в PyTorch является дочерним классом от [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Таким образом, нейронная сеть сама по себе будет являться *модулем*, состоящим из других *модулей* (слоев). Такая вложенная структура позволяет легко создавать сложные архитектуры и управлять ими.\n",
    "\n",
    "Сначала импортируем модуль (from torch import nn) и потом обращаемся к нему:\n",
    "функция активации ReLU --- nn.ReLU();\n",
    "функция активации sigmoid --- nn.Sigmoid();\n",
    "линейный слой --- nn.Linear();\n",
    "свёрточный слой --- nn.Conv2d() и т.д.\n",
    "\n",
    "Рассмотрим основные \"строительные блоки\", при помощи которых мы будем создавать нейронные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подмодуль torch.nn.Module --- описание класса модели\n",
    "\n",
    "Мы определяем нейронную сеть, наследуясь от класса `nn.Module`, и инициализируем ее слои в методе `__init__`. Каждый класс-наследник `nn.Module` производит операции над входными данными в методе `forward`.\n",
    "\n",
    "Множество слоев в нейронных сетях имеют *обучаемые параметры*, т. е. имеют ассоциированные с ними веса и смещения, которые оптимизируются во время обучения.\n",
    "\n",
    "Наследование от `nn.Module` автоматически отслеживает все слои, определенные внутри вашего класса модели, и делает все их параметры доступными с помощью методов `model.parameters()` или `model.named_parameters()`.\n",
    "\n",
    "Создадим на базе класса `nn.Module` свой собственный класс, который будет его наследником. В качестве объекта возьмём нейронную сеть из предыдущего раздела:\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=\"1000\">\n",
    "\n",
    "Эта простая нейронная сеть состоит из двух линейных слоёв: первый размером $3 \\times 2$ с последующим применением функции активации ReLU, второй размером $2 \\times 1$. Обращаю внимание на метод forward() --- именно он отвечает за \"проход по нейронной сети\". Строгое определение: процесс передачи значений от входных нейронов к выходным называется прямым распространением (forward pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала подключим необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потом создадим класс через наследование от класса `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(3, 2) ## WRITE YOUR CODE HERE\n",
    "        self.output = nn.Linear(2, 1) ## WRITE YOUR CODE HERE\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # функция прохода входного изображения сквозь нейронную сеть.\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x ## WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать модель, необходимо передать ей входные данные. Это приводит в действие метод `forward`, а также определенные фоновые операции. Не следует вызывать `model.forward` напрямую!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим случайный входной вектор размерности $1 \\times 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5856, 0.0539, 0.7208])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand(3) ## WRITE YOUR CODE HERE\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы начать работу с построенной нейронной сетью, необходимо сначала создать экземпляр описанного класса Network()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обращении к этому классу получим описание структуры созданной нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=3, out_features=2, bias=True)\n",
       "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь \"пропустим\" через полученную нейронную сеть созданный нами входной вектор input, выведем на экран полученный результат и его размерность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6844], grad_fn=<AddBackward0>)\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "result = model(input)\n",
    "print(result)\n",
    "print(result.shape)  ## WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку выходное множество состоит из одного элемента, извлечём его явное значение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6843937039375305"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WRITE YOUR CODE HERE\n",
    "result.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подмодули для функций активации\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L05/out/neurons_output.png\" width=\"1000\">\n",
    "\n",
    "Функции активации должны обладать следующими свойствами:\n",
    "\n",
    "* **Нелинейность:** функция активации необходима для введения нелинейности в нейронные сети. Если функция активации не применяется, выходной сигнал становится простой линейной функцией. Нейронная сеть без нелинейностей будет действовать как линейная модель с ограниченной способностью к обучению:\n",
    "$$\\hat{y}=NN(X,W_1,...,W_n)=X\\cdot W_1\\cdot ...\\cdot W_n=X\\cdot W$$\n",
    "Только нелинейные функции активации позволяют нейронным сетям решать задачи аппроксимации нелинейных функций:\n",
    "$$\\hat{y}=NN(X,W_1,...,W_n)=\\sigma(...\\sigma(X\\cdot W_1)...\\cdot W_n)\\neq X\\cdot W$$\n",
    "\n",
    "* **Дифференцируемость:** функции активации должны быть способными пропускать градиент, чтобы было возможно оптимизировать параметры сети градиентными методами, в частности использовать алгоритм обратного распространения ошибки.\n",
    "\n",
    "\n",
    "Любая функция активации задаётся просто своим названием: nn.ReLU, nn.Tanh, nn.Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подмодуль для линейного слоя nn.Linear\n",
    "\n",
    "Линейный слой [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) — это модуль, который производит линейное преобразование входных данных с помощью хранящихся в нем весов и смещений.\n",
    "\n",
    "Обязательными параметрами при объявлении этого слоя являются `in_features` — количество входных признаков, и `out_features` — количество выходных признаков.\n",
    "\n",
    "Фактически, этот модуль добавляет в модель один полносвязный слой нейронов *без активаций*. Слой состоит из `out_features` нейронов, каждый из которых имеет `in_features` входов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подмодуль для объединения слоёв в одну последовательность nn.Sequential\n",
    "\n",
    "[`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) — это упорядоченный контейнер для модулей. Данные проходят через все модули в том же порядке, в котором они определены в `nn.Sequential`. Можно использовать такой контейнер для того, чтобы быстро собрать простую нейронную сеть, что мы и сделаем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подмодуль nn.Flatten\n",
    "\n",
    "Мы используем слой nn.Flatten для преобразования каждого изображения 1×28×28 пикселей в непрерывный массив из 784 значений (размер батча (на позиции dim=0) сохраняется)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подмодуль для результирующего слоя nn.Softmax\n",
    "\n",
    "Последний линейный слой нейронной сети возвращает *логиты* — \"сырые\" значения из диапазона $[-∞; +∞]$, которые могут быть пропущены через модуль [`nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html). Пропущенные через $\\text{sofmax}$ величины могут восприниматься как вероятности, с которыми модель относит данный объект к тому или иному классу. Параметр `dim` определяет размерность, вдоль которой величины должны суммироваться к $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример применения всех описанных подмодулей\n",
    "\n",
    "Для иллюстрации возьмем мини-батч (в данном случае представленный тензором, заполненным случайными числами) из трех одноканальных изображений $28 \\times 28$ и посмотрим, что с ним происходит, когда мы пропускаем его через сеть.\n",
    "Обращаю внимание, что изображения хранятся в разных форматах, здесь будем использовать размерность (batch_size, channels, height, width).\n",
    "Итак, создаём тензор указанной размерности и выводим его размер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([3, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = torch.rand(3, 1, 28, 28) ## WRITE YOUR CODE HERE\n",
    "print(f\"Input size: {sample_batch.shape}\") ## WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вытянем наши тестовые изображения в один вектор и выведем его размер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after Flatten: torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(sample_batch)\n",
    "print(f\"Size after Flatten: {flat_image.size()}\") ## WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее объявим линейный слой из 512 нейронов, каждый из которых получает \"вытянутое\" изображение из 784 пикселей. Пропустим через него это изображение и выведем на экран его размерность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after Linear:  torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=784, out_features=512)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(f\"Size after Linear:  {hidden1.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейный слой, в отличие от слоя `nn.Flatten`, имеет обучаемые параметры — веса и смещения. Они хранятся как объекты специального класса `torch.nn.parameter.Parameter` и содержат в себе тензоры собственно с величинами параметров. Получить доступ к ним можно, обратившись к атрибутам слоя `.weight` и `.bias` соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of linear layer weights: torch.Size([512, 784])\n",
      "Type of linear layer weights: <class 'torch.nn.parameter.Parameter'>\n",
      "Size of linear layer biases: torch.Size([512])\n",
      "Type of linear layer biases: <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of linear layer weights: {layer1.weight.size()}\")\n",
    "print(f\"Type of linear layer weights: {type(layer1.weight)}\")\n",
    "print(f\"Size of linear layer biases: {layer1.bias.size()}\")\n",
    "print(f\"Type of linear layer biases: {type(layer1.bias)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь применим функцию ReLU к выходному значению линейного слоя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.3981,  0.4645,  0.4113,  ...,  0.0381,  0.1113, -0.3310],\n",
      "        [-0.4250,  0.8166, -0.2638,  ...,  0.0610,  0.1025, -0.1798],\n",
      "        [-0.3379,  0.5525,  0.1335,  ...,  0.0653,  0.1156, -0.1660]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "After ReLU: tensor([[0.0000, 0.4645, 0.4113,  ..., 0.0381, 0.1113, 0.0000],\n",
      "        [0.0000, 0.8166, 0.0000,  ..., 0.0610, 0.1025, 0.0000],\n",
      "        [0.0000, 0.5525, 0.1335,  ..., 0.0653, 0.1156, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Size after ReLU: torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "activations1 = nn.ReLU()(hidden1)\n",
    "\n",
    "print(f\"Before ReLU: {hidden1}\")\n",
    "print(f\"After ReLU: {activations1}\")\n",
    "print(f\"Size after ReLU: {activations1.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все описанные выше слои можно задать одной последовательностью при помощи подмодуля nn.Sequential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(flatten, layer1, nn.ReLU(), nn.Linear(512, 10))\n",
    "\n",
    "sample_batch = torch.rand(3, 1, 28, 28)\n",
    "logits = seq_modules(sample_batch)\n",
    "\n",
    "print(f\"Output size: {logits.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в завершение применим функцию nn.Softmax для предсказания вероятностей выходных классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after Softmax: torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits) ## WRITE YOUR CODE HERE\n",
    "\n",
    "print(f\"Size after Softmax: {pred_probab.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примере ниже мы проходимся по всем параметрам модели, и для каждого тензора параметров выводим его размер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Network(\n",
      "  (hidden): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "Layer: hidden.weight             | Size: torch.Size([2, 3])\n",
      "Layer: hidden.bias               | Size: torch.Size([2])\n",
      "Layer: output.weight             | Size: torch.Size([1, 2])\n",
      "Layer: output.bias               | Size: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name:25} | Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание: написать архитектуру нейронной сети, на вход которой подаётся изображение размера $28 \\times 28$ с одним скрытым линейным слоем, состоящим из 256 нейронов с функцией активации Sigmoid и выходным слоем, состоящим из 10 нейронов. К выходному слою применить функцию Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0848, 0.0857, 0.1110, 0.1185, 0.0932, 0.0964, 0.0678, 0.0987, 0.1406,\n",
       "         0.1034]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "input = torch.randn((1, 784))\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(784, 256) ## WRITE YOUR CODE HERE\n",
    "        self.output = nn.Linear(256, 10) ## WRITE YOUR CODE HERE\n",
    "        self.sigmoid = nn.Sigmoid() ## WRITE YOUR CODE HERE\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x) ## WRITE YOUR CODE HERE\n",
    "        x = self.sigmoid(x) ## WRITE YOUR CODE HERE\n",
    "        x = self.output(x) ## WRITE YOUR CODE HERE\n",
    "        x = self.softmax(x) ## WRITE YOUR CODE HERE\n",
    "        return x\n",
    "\n",
    "net = Network()\n",
    "net.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор устройства (device) для обучения\n",
    "\n",
    "Мы бы хотели иметь возможность обучать модель на аппаратном ускорителе, таком как GPU, если он доступен. Проверим, доступен ли нам ускоритель [`torch.cuda`](https://pytorch.org/docs/stable/notes/cuda.html), иначе продолжим вычисления на CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание: создать нейронную сеть, указанную на картинке:\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width='1000'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## WRITE YOUR CODE HERE\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## WRITE YOUR CODE HERE\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запишите эту же нейронную сеть с использованием модуля [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model1 = nn.Sequential( ## WRITE YOUR CODE HERE\n",
    "    nn.Linear(784, 128), ## WRITE YOUR CODE HERE\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),## WRITE YOUR CODE HERE\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),## WRITE YOUR CODE HERE\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания нейронной сети можно также пользоваться модулем [OrderedDict](https://docs.python.org/3/library/collections.html#collections.OrderedDict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model2 = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('fc1', nn.Linear(784, 128)), ## WRITE YOUR CODE HERE\n",
    "        ('relu1', nn.ReLU()), ## WRITE YOUR CODE HERE\n",
    "        ('fc2', nn.Linear(128, 64)), ## WRITE YOUR CODE HERE\n",
    "        ('relu2', nn.ReLU()), ## WRITE YOUR CODE HERE\n",
    "        ('output', nn.Linear(64, 10)), ## WRITE YOUR CODE HERE\n",
    "        ('softmax', nn.Softmax(dim=1)) ## WRITE YOUR CODE HERE\n",
    "    ])\n",
    ")\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы явно увидеть веса модели model2, надо использовать метод parameters():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0148, -0.0159, -0.0177,  ..., -0.0031,  0.0329,  0.0342],\n",
      "        [ 0.0087,  0.0344,  0.0239,  ..., -0.0004, -0.0250,  0.0246],\n",
      "        [ 0.0166,  0.0317,  0.0116,  ..., -0.0295, -0.0273,  0.0006],\n",
      "        ...,\n",
      "        [-0.0195, -0.0142, -0.0098,  ...,  0.0075, -0.0052, -0.0216],\n",
      "        [ 0.0005, -0.0329,  0.0113,  ...,  0.0038, -0.0293, -0.0216],\n",
      "        [ 0.0032,  0.0187, -0.0204,  ..., -0.0060, -0.0347, -0.0020]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0118, -0.0138,  0.0275, -0.0349,  0.0232, -0.0122, -0.0173, -0.0296,\n",
      "         0.0191,  0.0061, -0.0267, -0.0054, -0.0206,  0.0025,  0.0136, -0.0025,\n",
      "        -0.0347, -0.0075, -0.0116,  0.0203, -0.0312,  0.0098, -0.0287,  0.0156,\n",
      "        -0.0245,  0.0155,  0.0328, -0.0015, -0.0264,  0.0134, -0.0307, -0.0040,\n",
      "        -0.0038, -0.0125,  0.0277, -0.0172,  0.0148,  0.0296, -0.0226, -0.0146,\n",
      "        -0.0259, -0.0031, -0.0045, -0.0290,  0.0009, -0.0102,  0.0103, -0.0183,\n",
      "         0.0057, -0.0144, -0.0195,  0.0193, -0.0045, -0.0136,  0.0072,  0.0137,\n",
      "        -0.0008, -0.0104,  0.0354, -0.0318, -0.0320, -0.0279,  0.0323, -0.0187,\n",
      "        -0.0259, -0.0315, -0.0102,  0.0080, -0.0070,  0.0283,  0.0270,  0.0161,\n",
      "        -0.0316, -0.0282,  0.0049,  0.0170,  0.0053, -0.0225, -0.0321,  0.0189,\n",
      "         0.0235,  0.0135,  0.0346,  0.0137, -0.0287,  0.0199,  0.0271,  0.0292,\n",
      "        -0.0318, -0.0298,  0.0333, -0.0153,  0.0092,  0.0235, -0.0346,  0.0215,\n",
      "        -0.0276, -0.0262, -0.0044, -0.0243,  0.0127,  0.0351, -0.0071, -0.0101,\n",
      "        -0.0356, -0.0234, -0.0342, -0.0235,  0.0094,  0.0147,  0.0115,  0.0142,\n",
      "        -0.0333, -0.0007,  0.0218, -0.0346, -0.0202, -0.0352, -0.0094, -0.0190,\n",
      "        -0.0008, -0.0041,  0.0145,  0.0213,  0.0183, -0.0116,  0.0353, -0.0058],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## WRITE YOUR CODE HERE\n",
    "for i in model2.fc1.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
